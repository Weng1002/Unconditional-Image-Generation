{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c313ac8",
   "metadata": {},
   "source": [
    "# 先爬2024年表特版所有的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 照片處理\n",
    "from urllib.parse import urlparse\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b57692",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    'User-Agent':'Mozilla/5.0',\n",
    "    'cookie':'over18=1'\n",
    "}\n",
    "\n",
    "if os.path.exists('2020_articles.jsonl'):\n",
    "    os.remove('2020_articles.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e61abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta_value(soup, label):\n",
    "    tags = soup.select('span.article-meta-tag')\n",
    "    vals = soup.select('span.article-meta-value')\n",
    "\n",
    "    for tag, val in zip(tags, vals):\n",
    "        if tag.text.strip() == label:\n",
    "            return val.text.strip()\n",
    "    \n",
    "    # 處理特殊情況，沒有時間欄位時\n",
    "    if label == '時間':\n",
    "        f2_texts = [span.text.strip() for span in soup.select('span.f2')]\n",
    "\n",
    "        for line in f2_texts:\n",
    "            match = re.search(r'(\\d{2}/\\d{2}/\\d{4})\\s+(\\d{2}:\\d{2}:\\d{2})', line)\n",
    "            if match:\n",
    "                date_part = match.group(1)  \n",
    "                time_part = match.group(2)  \n",
    "                dt = datetime.strptime(date_part + ' ' + time_part, '%m/%d/%Y %H:%M:%S')\n",
    "                return dt.strftime('%a %b %d %H:%M:%S %Y')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def crawl_articles():\n",
    "    is_2024_started = False\n",
    "    start_index = 2329  #3911(2025)  3647(2024)   3367(2023)    2712(2021)  2329(2020)\n",
    "    end_index = 2712   #4001(2025)  3916(2024)   3647(2023)   3060(2021)   2712(2020)\n",
    "\n",
    "    for index in range(start_index, end_index+1):  \n",
    "        url = f'https://www.ptt.cc/bbs/Beauty/index{index}.html'\n",
    "        print(\"\\n\")\n",
    "        print(f'目前的列表: {url}')\n",
    "\n",
    "        res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status() # 檢查是否取得成功\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        entries = soup.select('div.r-ent') # 取得文章列表\n",
    "        for entry in entries:\n",
    "            link_tag = entry.select_one('a')\n",
    "            if not link_tag:\n",
    "                continue  # 無網址的文章\n",
    "\n",
    "            post_url = 'https://www.ptt.cc' + link_tag['href']\n",
    "            title_text = link_tag.text.strip()\n",
    "            date_tag = entry.select_one('div.date')\n",
    "            post_date = date_tag.text.strip() if date_tag else ''\n",
    "            print(f'抓到的文章：{post_url} | 標題：{title_text} | 列表時間：{post_date}')\n",
    "            \n",
    "\n",
    "            # 還沒進入 2024，先用舊邏輯\n",
    "            if not is_2024_started:\n",
    "                res_post = requests.get(post_url, headers=HEADERS, timeout=10)\n",
    "                res_post.raise_for_status()\n",
    "                post_soup = BeautifulSoup(res_post.text, 'html.parser')\n",
    "                post_time = extract_meta_value(post_soup, '時間')\n",
    "                if not post_time:\n",
    "                    print(f\"無法解析時間\")\n",
    "                    continue\n",
    "                dt = datetime.strptime(post_time, '%a %b %d %H:%M:%S %Y')\n",
    "\n",
    "                if dt.year < 2020:\n",
    "                    print('跳過早於 2024 年的文章')\n",
    "                    continue\n",
    "                elif dt.year > 2020:\n",
    "                    print('跳過 2025 年的文章')\n",
    "                    continue\n",
    "\n",
    "                # 確認已進入 2024 年\n",
    "                is_2024_started = True\n",
    "                mmdd = dt.strftime('%m%d')\n",
    "            else:\n",
    "                # 已進入 2024，直接從列表時間推斷\n",
    "                if post_date >= '01/01':\n",
    "                    if res_post.status_code == 404:\n",
    "                        print(\"該文章為 404，略過\")\n",
    "                        continue\n",
    "                    post_soup = BeautifulSoup(res_post.text, 'html.parser')\n",
    "                    post_time = extract_meta_value(post_soup, '時間')\n",
    "                    if not post_time:\n",
    "                        continue\n",
    "                    dt = datetime.strptime(post_time, '%a %b %d %H:%M:%S %Y')\n",
    "\n",
    "                    if dt.year == 2021:\n",
    "                        print(\"抓到 2025 年文章，結束爬蟲\")\n",
    "                        return\n",
    "                    elif dt.year != 2020:\n",
    "                        continue        \n",
    "                    mmdd = dt.strftime('%m%d')\n",
    "                else:\n",
    "                    mmdd = post_date.replace(\"/\", \"\").zfill(4)\n",
    "\n",
    "            # 篩選\n",
    "            if not title_text.strip():\n",
    "                print(f'略過標題為空白或空字串')\n",
    "                continue\n",
    "            if '[公告]' in title_text or 'Fw:[公告]' in title_text:\n",
    "                print('略過公告文')\n",
    "                continue\n",
    "            \n",
    "            article_data = {\n",
    "                'date': mmdd,\n",
    "                'title': title_text,\n",
    "                'url': post_url\n",
    "            }\n",
    "            \n",
    "            with open('2020_articles.jsonl', 'a', encoding='utf-8') as fa:\n",
    "                fa.write(json.dumps(article_data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "            \n",
    "            time.sleep(random.uniform(0.3, 0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee12188",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab40919",
   "metadata": {},
   "source": [
    "# 只抓取有\"正妹\"的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_urls(text):\n",
    "    pattern = r'https?://[^\\s\"]+\\.(?:jpg|jpeg|png|gif)(?=\\b|$)'\n",
    "    return re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "def Keyword(keyword: str):\n",
    "    target_articles = []\n",
    "    image_urls = []\n",
    "\n",
    "    with open('2020_articles.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            article = json.loads(line)\n",
    "\n",
    "            if keyword in article[\"title\"]: # 標題包含關鍵字\n",
    "                target_articles.append(article)\n",
    "                continue\n",
    "\n",
    "            # 否則檢查內文是否包含關鍵字\n",
    "            url = article[\"url\"]\n",
    "            res = requests.get(url, headers=HEADERS, timeout=40)\n",
    "            res.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            main_content = soup.select_one(\"#main-content\")\n",
    "            if not main_content:\n",
    "                continue\n",
    "\n",
    "            text = main_content.get_text(separator=\"\\n\")\n",
    "            content_split = text.split(\"※ 發信站\")\n",
    "            if len(content_split) < 2:\n",
    "                continue\n",
    "            content = content_split[0]\n",
    "\n",
    "            if keyword in content:\n",
    "                target_articles.append(article)\n",
    "\n",
    "    print(f\"找到 {len(target_articles)} 篇文章（標題或內文含關鍵字「{keyword}」）\")\n",
    "\n",
    "    for article in tqdm(target_articles, desc=\"處理特定文章\"):\n",
    "        url = article[\"url\"]\n",
    "        print(f\"處理中：{url}\")\n",
    "        try:\n",
    "            res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            res.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"[!] 無法下載文章內容：{e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        main_content = soup.select_one(\"#main-content\")\n",
    "        if not main_content:\n",
    "            continue\n",
    "\n",
    "        text = main_content.get_text(separator=\"\\n\")\n",
    "        content_split = text.split(\"※ 發信站\")\n",
    "        if len(content_split) < 2:\n",
    "            print(\"無法找到發信站標記，跳過\")\n",
    "            continue\n",
    "\n",
    "        content = content_split[0]\n",
    "\n",
    "        print(\"符合條件，開始擷取圖片連結\")\n",
    "\n",
    "        pushes = soup.select(\"div.push span.push-content\")\n",
    "        for push in pushes:\n",
    "            content += push.text\n",
    "\n",
    "        image_urls += extract_image_urls(content)\n",
    "        time.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "    unique_image_urls = list(set(image_urls))\n",
    "\n",
    "    result = {\n",
    "        \"image_urls\": unique_image_urls\n",
    "    }\n",
    "\n",
    "    outname = f\"2020_keyword_{keyword}.json\"\n",
    "    with open(outname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"完成 keyword 抽圖：{outname}，共 {len(unique_image_urls)} 張圖片\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Keyword(\"正妹\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79f755",
   "metadata": {},
   "source": [
    "# 將剛剛的圖片url進行下載保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../raw_images/2020_正妹_images'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open('2020_keyword_正妹.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    image_urls = data.get(\"image_urls\", [])\n",
    "\n",
    "print(f\"共載入 {len(image_urls)} 張圖片網址，開始下載\")\n",
    "\n",
    "# 開始下載圖片\n",
    "for idx, url in enumerate(tqdm(image_urls, desc=\"Downloading\")):\n",
    "    if url.startswith(\"https://d.img.vision/dddshay/\"):\n",
    "        print(f\"[!] 跳過：{url}\")\n",
    "        continue\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            timeout=35,\n",
    "            headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "                'Referer': 'https://www.google.com/'\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        filename = f\"{idx:05d}.png\" # 儲存為 PNG 格式\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        image.save(filepath, format=\"PNG\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] 第 {idx} 張圖片處理失敗：{url} | 錯誤：{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
